{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08e9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def run_training():\n",
    "    import os\n",
    "    import torch\n",
    "    from datasets import load_from_disk\n",
    "    from transformers import (\n",
    "        ModernBertConfig,\n",
    "        ModernBertForMaskedLM,\n",
    "        DataCollatorForLanguageModeling,\n",
    "        AutoTokenizer,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "    )\n",
    "    from accelerate import Accelerator\n",
    "\n",
    "    # Initialize Accelerator. The Trainer will automatically detect and use it.\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    # --- 1. Setup Paths and Environment ---\n",
    "    WORK_DIR = os.getenv('WORK')\n",
    "    DATA_FOLDER = os.path.join(WORK_DIR, \"data\")\n",
    "    CACHED_DATA_FOLDER = os.path.join(WORK_DIR, \"cached_data\")\n",
    "    os.environ['HF_HOME'] = CACHED_DATA_FOLDER\n",
    "    os.environ['TRITON_HIP_LLD_PATH'] = '/opt/rocm-6.4.1/lib/llvm/bin/ld.lld'\n",
    "    os.chdir(WORK_DIR)\n",
    "    \n",
    "    accelerator.print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "    # --- 2. Define Parameters ---\n",
    "    vocabulary_size = 32_768\n",
    "    context_size = 512\n",
    "    tokenizer_name = f\"tokenizers/custom/{vocabulary_size:_}\"\n",
    "    model_name = f\"Modern/{4.6}\"\n",
    "\n",
    "    # --- 3. Load Datasets and Tokenizer ---\n",
    "    tokenized_datasets_name = os.path.join(DATA_FOLDER, f\"tokenized-for-training/custom/vocab_size:{vocabulary_size:_}/context_size:{context_size}\")\n",
    "    tokenized_datasets = load_from_disk(tokenized_datasets_name)\n",
    "    training_dataset = tokenized_datasets[\"train\"]\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name,\n",
    "        local_files_only=True,\n",
    "        cache_dir=CACHED_DATA_FOLDER\n",
    "    )\n",
    "\n",
    "    # --- 4. Configure and Initialize Model ---\n",
    "    config = ModernBertConfig.from_pretrained(\n",
    "        \"answerdotai/ModernBERT-base\",\n",
    "        reference_compile=False,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    config.vocab_size = vocabulary_size\n",
    "    config.max_position_embeddings = 512\n",
    "    config.local_attention = 128\n",
    "    config.pad_token_id = 0\n",
    "    config.bos_token_id = 2\n",
    "    config.cls_token_id = 2\n",
    "    config.eos_token_id = 3\n",
    "    config.sep_token_id = 3\n",
    "\n",
    "    model = ModernBertForMaskedLM(config=config)\n",
    "    # NOTE: We do NOT call model.to(\"cuda\") or model.half().\n",
    "    # The Trainer, powered by Accelerate, will handle device placement and mixed precision.\n",
    "\n",
    "    # --- 5. Setup Collator and Training Arguments ---\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=True,\n",
    "        mlm_probability=0.3\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'training/{model_name}',\n",
    "        overwrite_output_dir=True,\n",
    "        max_steps=500_000,\n",
    "        per_device_train_batch_size=256,   # This is now PER GPU\n",
    "        gradient_accumulation_steps=1,\n",
    "        dataloader_num_workers=64,         # Correct way to set workers with Trainer\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_first_step=True,\n",
    "        logging_steps=1_000,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=1_000,\n",
    "        save_total_limit=5,\n",
    "        fp16=True,                         # Enable mixed precision\n",
    "    )\n",
    "\n",
    "    # --- 6. Initialize and Run Trainer ---\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    accelerator.print(\"Starting training on all available GPUs...\")\n",
    "    trainer.train()\n",
    "    accelerator.print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417d1aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 CUDAs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "W0926 20:30:24.722184 1573209 torch/multiprocessing/spawn.py:175] Terminating process 1573303 via signal SIGTERM\n",
      "W0926 20:30:24.724163 1573209 torch/multiprocessing/spawn.py:175] Terminating process 1573305 via signal SIGTERM\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742] failed (exitcode: 1) local_rank: 0 (pid: 1573301) of fn: run_training (start_method: fork)\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742] Traceback (most recent call last):\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]   File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 697, in _poll\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]     self._pc.join(-1)\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]   File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 221, in join\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]     raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742] torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742] \n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742] -- Process 0 terminated with the following error:\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742] Traceback (most recent call last):\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]   File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 96, in _wrap\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]     fn(i, *args)\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]   File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 617, in _wrap\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]     ret = record(fn)(*args_)\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]   File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]     return f(*args, **kwargs)\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]   File \"/tmp/ipykernel_1573209/2636644470.py\", line 17, in run_training\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]     accelerator = Accelerator()\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]   File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/accelerate/accelerator.py\", line 462, in __init__\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]     self.state = AcceleratorState(\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]   File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/accelerate/state.py\", line 912, in __init__\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]     PartialState(cpu, **kwargs)\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]   File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/accelerate/state.py\", line 301, in __init__\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]     self.set_device()\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]   File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/accelerate/state.py\", line 838, in set_device\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]     device_module.set_device(self.device)\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]   File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/cuda/__init__.py\", line 567, in set_device\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]     torch._C._cuda_setDevice(device)\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]   File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/cuda/__init__.py\", line 398, in _lazy_init\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742]     raise RuntimeError(\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "E0926 20:30:24.819444 1573209 torch/distributed/elastic/multiprocessing/api.py:742] \n"
     ]
    },
    {
     "ename": "ChildFailedError",
     "evalue": "\n============================================================\nrun_training FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-09-26_20:30:24\n  host      : k005-009.hpcfund\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 1573301)\n  error_file: /tmp/torchelastic_eenny7rl/none_v2l0bsfr/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_1573209/2636644470.py\", line 17, in run_training\n      accelerator = Accelerator()\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/accelerate/accelerator.py\", line 462, in __init__\n      self.state = AcceleratorState(\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/accelerate/state.py\", line 912, in __init__\n      PartialState(cpu, **kwargs)\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/accelerate/state.py\", line 301, in __init__\n      self.set_device()\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/accelerate/state.py\", line 838, in set_device\n      device_module.set_device(self.device)\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/cuda/__init__.py\", line 567, in set_device\n      torch._C._cuda_setDevice(device)\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/cuda/__init__.py\", line 398, in _lazy_init\n      raise RuntimeError(\n  RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n  \n============================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mChildFailedError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m notebook_launcher\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# This will launch the function you defined above on 4 GPUs.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# The notebook will wait here until the training is finished.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/accelerate/launchers.py:247\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[1;32m    246\u001b[0m         launch_config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_line_prefix_template\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_line_prefix_template\n\u001b[0;32m--> 247\u001b[0m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_type\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/distributed/launcher/api.py:162\u001b[0m, in \u001b[0;36melastic_launch.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/distributed/launcher/api.py:299\u001b[0m, in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    292\u001b[0m     events\u001b[38;5;241m.\u001b[39mrecord(agent\u001b[38;5;241m.\u001b[39mget_event_succeeded(), config\u001b[38;5;241m.\u001b[39mevent_log_handler)\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mis_failed():\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;66;03m# ChildFailedError is treated specially by @record\u001b[39;00m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;66;03m# if the error files for the failed children exist\u001b[39;00m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# @record will copy the first error (root cause)\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;66;03m# to the error file of the launcher process.\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChildFailedError(\n\u001b[1;32m    300\u001b[0m             name\u001b[38;5;241m=\u001b[39mentrypoint_name,\n\u001b[1;32m    301\u001b[0m             failures\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mfailures,\n\u001b[1;32m    302\u001b[0m         )\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturn_values\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ChildFailedError:\n",
      "\u001b[0;31mChildFailedError\u001b[0m: \n============================================================\nrun_training FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-09-26_20:30:24\n  host      : k005-009.hpcfund\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 1573301)\n  error_file: /tmp/torchelastic_eenny7rl/none_v2l0bsfr/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_1573209/2636644470.py\", line 17, in run_training\n      accelerator = Accelerator()\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/accelerate/accelerator.py\", line 462, in __init__\n      self.state = AcceleratorState(\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/accelerate/state.py\", line 912, in __init__\n      PartialState(cpu, **kwargs)\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/accelerate/state.py\", line 301, in __init__\n      self.set_device()\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/accelerate/state.py\", line 838, in set_device\n      device_module.set_device(self.device)\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/cuda/__init__.py\", line 567, in set_device\n      torch._C._cuda_setDevice(device)\n    File \"/work1/lgarcia/renneruan/amd_200/lib64/python3.9/site-packages/torch/cuda/__init__.py\", line 398, in _lazy_init\n      raise RuntimeError(\n  RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n  \n============================================================"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "# This will launch the function you defined above on 4 GPUs.\n",
    "# The notebook will wait here until the training is finished.\n",
    "notebook_launcher(run_training, num_processes=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd_200",
   "language": "python",
   "name": "amd_200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
