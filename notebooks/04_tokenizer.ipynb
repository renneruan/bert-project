{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12468353",
   "metadata": {},
   "source": [
    "# Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c9390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k003-002.hpcfund\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b54e7979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work1/lgarcia/renneruan/cached_data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "WORK_DIR = os.getenv('WORK')\n",
    "\n",
    "DATA_FOLDER = os.path.join(WORK_DIR, \"data\")\n",
    "\n",
    "CACHED_DATA_FOLDER = os.path.join(WORK_DIR, \"cached_data\")\n",
    "\n",
    "# Salvamos o path do Cache par ao HuggingFace\n",
    "os.environ['HF_HOME'] = CACHED_DATA_FOLDER\n",
    "\n",
    "CACHED_DATA_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a01abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work1/lgarcia/renneruan\n"
     ]
    }
   ],
   "source": [
    "os.chdir(WORK_DIR)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13376096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers.normalizers import NFC, Lowercase, Replace\n",
    "from tokenizers import (\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    Tokenizer\n",
    ")\n",
    "\n",
    "from tokenizers.pre_tokenizers import Punctuation, Metaspace, WhitespaceSplit, Digits\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.decoders import Metaspace\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438b2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 32_768\n",
    "context_size = 512\n",
    "tokenizer_name = f\"tokenizers/custom/{vocabulary_size:_}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ccf8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_save_path = os.path.join(DATA_FOLDER, 'split_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54258c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = load_from_disk(split_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a25d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered by the index\n",
    "custom_special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3e69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc3\\x85' b'\\xe2\\x84\\xab'\n",
      "b'oi, \\xc3\\xa5 \\xc3\\xa5 tudo bem? o \"qu\\xc3\\xaa\" \"essa\" \"fun\\xc3\\xa7\\xc3\\xa3o\" faz?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'oi, √• √• tudo bem? o \"qu√™\" \"essa\" \"fun√ß√£o\" faz?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace latex usage of \"aspas\"\n",
    "# NFC - Canonico (evita equivalencia) e composto, menos characteres\n",
    "# lowercase - unifica maiuscula e mi nusculo, que geralmente muda pouco (o positional embedding pode cuidar de comecar com maiuscula)\n",
    "\n",
    "# NFC, NFK\n",
    "# √É A~\n",
    "\n",
    "custom_normalizer = normalizers.Sequence([\n",
    "    Replace(\"``\", '\"'), \n",
    "    Replace(\"''\", '\"'),\n",
    "    NFC(), \n",
    "    Lowercase(),\n",
    "])\n",
    "\n",
    "print(\"√Ö\".encode(\"utf-8\"), \"‚Ñ´\".encode(\"utf-8\"))\n",
    "\n",
    "normalized_sample = custom_normalizer.normalize_str(\"Oi, √Ö ‚Ñ´ tudo BEM? O ``qu√™'' \\\"essa\\\" ''fun√ß√£o'' faz?\")\n",
    "\n",
    "print(normalized_sample.encode(\"utf-8\"))\n",
    "\n",
    "normalized_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9990d201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‚ñÅQuer', (0, 4)),\n",
       " ('‚ñÅsaber', (5, 10)),\n",
       " ('‚ñÅ,', (10, 11)),\n",
       " ('‚ñÅvamos', (12, 17)),\n",
       " ('‚ñÅver', (18, 21)),\n",
       " ('‚ñÅo', (22, 23)),\n",
       " ('‚ñÅque', (24, 27)),\n",
       " ('‚ñÅesse', (28, 32)),\n",
       " ('‚ñÅpre', (33, 36)),\n",
       " ('‚ñÅ-', (36, 37)),\n",
       " ('‚ñÅtokenizer', (37, 46)),\n",
       " ('‚ñÅfaz', (47, 50)),\n",
       " ('‚ñÅcom', (51, 54)),\n",
       " ('‚ñÅnumeros', (55, 62)),\n",
       " ('‚ñÅ:', (62, 63)),\n",
       " ('‚ñÅ12345', (64, 69)),\n",
       " ('‚ñÅ123', (70, 73)),\n",
       " ('‚ñÅ192', (74, 77)),\n",
       " ('‚ñÅ193', (78, 81)),\n",
       " ('‚ñÅ!', (81, 82)),\n",
       " ('‚ñÅ?', (82, 83)),\n",
       " ('‚ñÅ!', (83, 84))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# to keep the original words (separed by whitespaces), we use metaspaces, represented by:\n",
    "# ‚Äú‚ñÅ‚Äù U+2581 Lower One Eighth Block Unicode Character\n",
    "\n",
    "# Digits(individual_digits=True), Whitespace()\n",
    "\n",
    "custom_pre_tokenizer = pre_tokenizers.Sequence([WhitespaceSplit(), Punctuation(), Digits(individual_digits=False), Metaspace(replacement=\"‚ñÅ\", prepend_scheme=\"always\")])\n",
    "\n",
    "custom_pre_tokenizer.pre_tokenize_str(\"Quer saber, vamos ver o que esse pre-tokenizer faz com numeros: 12345 123 192 193!?!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17c5cf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‚ñÅo', (0, 1)),\n",
       " ('‚ñÅqu√™', (2, 5)),\n",
       " ('‚ñÅele', (6, 9)),\n",
       " ('‚ñÅfaz', (10, 13)),\n",
       " ('‚ñÅcom', (16, 19)),\n",
       " ('‚ñÅespa√ßos', (28, 35)),\n",
       " ('‚ñÅadicionais', (36, 46)),\n",
       " ('‚ñÅe', (50, 51)),\n",
       " ('‚ñÅquebras', (52, 59)),\n",
       " ('‚ñÅde', (60, 62)),\n",
       " ('‚ñÅlinhas', (63, 69)),\n",
       " ('‚ñÅ?', (69, 70))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_pre_tokenizer.pre_tokenize_str(\"o qu√™ ele faz \\t com         espa√ßos adicionais \\n\\n e quebras de linhas?   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1078d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_post_processor = TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", custom_special_tokens.index(\"[CLS]\")), (\"[SEP]\", custom_special_tokens.index(\"[SEP]\"))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de7cc10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_decoder = Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tokenizer = Tokenizer(Unigram())\n",
    "\n",
    "custom_tokenizer.normalizer = custom_normalizer\n",
    "custom_tokenizer.pre_tokenizer = custom_pre_tokenizer\n",
    "custom_tokenizer.post_processor = custom_post_processor\n",
    "custom_tokenizer.decoder = custom_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45309cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443178/443178 [04:40<00:00, 1577.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_trainer = UnigramTrainer(\n",
    "        vocab_size=vocabulary_size,\n",
    "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "        special_tokens=custom_special_tokens,\n",
    "        unk_token=\"[UNK]\",\n",
    ")\n",
    "\n",
    "# create a python generator to dynamically load the data, one batch at a time\n",
    "def batch_iterator(batch_size=128): # 128 (cores)\n",
    "    for i in tqdm(range(0, len(split_dataset[\"train\"]), batch_size)):\n",
    "        yield split_dataset[\"train\"][i : i + batch_size][\"text\"]\n",
    "\n",
    "custom_tokenizer.train_from_iterator(iterator=batch_iterator(), trainer=custom_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dce87b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_custom_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=custom_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    bos_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    eos_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    padding_side=\"right\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af7673d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizers/custom/32_768/tokenizer_config.json',\n",
       " 'tokenizers/custom/32_768/special_tokens_map.json',\n",
       " 'tokenizers/custom/32_768/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save locally\n",
    "fast_custom_tokenizer.save_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9aeff8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = fast_custom_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1dbc70",
   "metadata": {},
   "source": [
    "## Teste do Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0189c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '‚ñÅol√°', '‚ñÅpessoal', '‚ñÅ', ',', '‚ñÅcomo', '‚ñÅvoc√™s', '‚ñÅest√£o', '‚ñÅ', 'üòÅ', '‚ñÅ', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# print( tokenizer_albert.convert_ids_to_tokens( tokenizer_albert.encode(\"Ol√° pessoal, Como voc√™s est√£o üòÅ ?\") ) )\n",
    "\n",
    "print( tokenizer.convert_ids_to_tokens( tokenizer.encode(\"Ol√° pessoal, Como voc√™s est√£o üòÅ ?\") ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31514bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 7, 16208, 5, 8, 2535, 24, 5553, 5, 6, 47, 5, 8, 5553, 32, 2151, 3]\n",
      "['[CLS]', '‚ñÅ', 'a', 'tiraram', '‚ñÅ', 'o', '‚ñÅpau', '‚ñÅno', '‚ñÅgato', '‚ñÅ', ',', '‚ñÅmas', '‚ñÅ', 'o', '‚ñÅgato', '‚ñÅn√£o', '‚ñÅmorreu', '[SEP]']\n",
      "\n",
      "[2, 32, 774, 5, 6, 104, 774, 13, 43, 113, 5, 12, 5, 12, 5, 12, 3]\n",
      "['[CLS]', '‚ñÅn√£o', '‚ñÅsei', '‚ñÅ', ',', '‚ñÅs√≥', '‚ñÅsei', '‚ñÅque', '‚ñÅfoi', '‚ñÅassim', '‚ñÅ', '.', '‚ñÅ', '.', '‚ñÅ', '.', '[SEP]']\n",
      "\n",
      "[2, 5, 16436, 5, 8, 427, 12153, 5, 6, 5, 11, 6102, 5, 8, 427, 587, 10929, 1272, 5, 31, 13588, 5, 30, 3]\n",
      "['[CLS]', '‚ñÅ', 'testando', '‚ñÅ', 'o', '‚ñÅmodo', '‚ñÅcontinuo', '‚ñÅ', ',', '‚ñÅ', 'e', '‚ñÅtambem', '‚ñÅ', 'o', '‚ñÅmodo', '‚ñÅsub', 'jun', 'tivo', '‚ñÅ', '(', '‚ñÅsoubesse', '‚ñÅ', ')', '[SEP]']\n",
      "\n",
      "[2, 3563, 5, 6, 4058, 5, 6, 378, 5, 6, 13011, 5, 6, 5, 19156, 5, 6, 2229, 5, 6, 480, 3]\n",
      "['[CLS]', '‚ñÅjusto', '‚ñÅ', ',', '‚ñÅjusta', '‚ñÅ', ',', '‚ñÅjusti√ßa', '‚ñÅ', ',', '‚ñÅinjusto', '‚ñÅ', ',', '‚ñÅ', 'injustamente', '‚ñÅ', ',', '‚ñÅjustamente', '‚ñÅ', ',', '‚ñÅjunto', '[SEP]']\n",
      "\n",
      "[2, 5, 16436, 5, 7, 2211, 8, 10, 5, 6, 178, 13, 644, 5, 8, 10, 5, 7, 2211, 8, 10, 161, 7, 5, 7, 5, 7, 2274, 9503, 14, 189, 535, 5, 101, 3]\n",
      "['[CLS]', '‚ñÅ', 'testando', '‚ñÅ', 'a', 'cent', 'o', 's', '‚ñÅ', ',', '‚ñÅser√°', '‚ñÅque', '‚ñÅmanter', '‚ñÅ', 'o', 's', '‚ñÅ', 'a', 'cent', 'o', 's', '‚ñÅmelhor', 'a', '‚ñÅ', 'a', '‚ñÅ', 'a', 'cur', '√°cia', '‚ñÅdo', '‚ñÅmeu', '‚ñÅmodelo', '‚ñÅ', '?', '[SEP]']\n",
      "\n",
      "[2, 1008, 5, 50, 5, 7, 23, 34, 9678, 5, 6, 2915, 5, 6, 5, 7, 23, 34, 13257, 5, 6, 5, 7, 5185, 158, 5, 6, 2915, 780, 8, 5, 6, 2915, 2326, 780, 8, 3]\n",
      "['[CLS]', '‚ñÅamigo', '‚ñÅ', ':', '‚ñÅ', 'a', 'm', 'i', 'guinho', '‚ñÅ', ',', '‚ñÅamiga', '‚ñÅ', ',', '‚ñÅ', 'a', 'm', 'i', 'guinha', '‚ñÅ', ',', '‚ñÅ', 'a', 'mig', '√£o', '‚ñÅ', ',', '‚ñÅamiga', '√ß', 'o', '‚ñÅ', ',', '‚ñÅamiga', 'lha', '√ß', 'o', '[SEP]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_tokenizer(sample):\n",
    "    encoding = tokenizer.encode(sample)\n",
    "    print(encoding)\n",
    "    print(tokenizer.convert_ids_to_tokens(encoding))\n",
    "    print()\n",
    "\n",
    "test_tokenizer('''\n",
    "Atiraram o pau no gato, mas o gato n√£o morreu\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "N√£o sei, s√≥ sei que foi assim...\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "Testando o modo continuo, e tambem o modo subjuntivo ( soubesse )\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "justo, justa, justi√ßa, injusto, injustamente, justamente, junto\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "    testando acentos, ser√° que manter os acentos melhora a acur√°cia do meu modelo? \n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "    amigo: amiguinho, amiga, amiguinha, amig√£o, amiga√ßo, amigalha√ßo\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d7db67",
   "metadata": {},
   "source": [
    "#### Objetively Evaluate Tokenizer on Compression Rate (1/Fertility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "478ee47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‚ñÅao', (0, 2)),\n",
       " ('‚ñÅfinal', (3, 8)),\n",
       " ('‚ñÅde', (9, 11)),\n",
       " ('‚ñÅmaio', (12, 16)),\n",
       " ('‚ñÅdeste', (17, 22)),\n",
       " ('‚ñÅano', (23, 26)),\n",
       " ('‚ñÅ,', (26, 27)),\n",
       " ('‚ñÅquase', (28, 33)),\n",
       " ('‚ñÅ100', (34, 37)),\n",
       " ('‚ñÅpessoas', (38, 45)),\n",
       " ('‚ñÅforam', (46, 51)),\n",
       " ('‚ñÅdetidas', (52, 59)),\n",
       " ('‚ñÅnuma', (60, 64)),\n",
       " ('‚ñÅinvestida', (65, 74)),\n",
       " ('‚ñÅglobal', (75, 81)),\n",
       " ('‚ñÅcontra', (82, 88)),\n",
       " ('‚ñÅos', (89, 91)),\n",
       " ('‚ñÅcriadores', (92, 101)),\n",
       " ('‚ñÅ,', (101, 102)),\n",
       " ('‚ñÅvendedores', (103, 113)),\n",
       " ('‚ñÅe', (114, 115)),\n",
       " ('‚ñÅusu√°rios', (116, 124)),\n",
       " ('‚ñÅdo', (125, 127)),\n",
       " ('‚ñÅblackshades', (128, 139)),\n",
       " ('‚ñÅrat', (140, 143)),\n",
       " ('‚ñÅ.', (143, 144))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_and_pre_tokenize(text):\n",
    "    normalized = tokenizer.backend_tokenizer.normalizer.normalize_str(text)\n",
    "    processed = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(normalized)\n",
    "    return processed\n",
    "\n",
    "normalize_and_pre_tokenize( split_dataset[\"train\"][0][\"text\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dabcc092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b534ced9464125a57137d151d0b4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/56726693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def count_tokens(batch):\n",
    "    \n",
    "    original_tokens = 0\n",
    "    generated_tokens = 0\n",
    "\n",
    "    for doc in batch[\"text\"]:\n",
    "\n",
    "        original_tokens += len( normalize_and_pre_tokenize(doc) )\n",
    "        generated_tokens += len( tokenizer.encode(doc) )\n",
    "        \n",
    "    # Add the token counts as a new column to the batch\n",
    "    return {\n",
    "        \"generated\": [generated_tokens],\n",
    "        \"original\": [original_tokens]\n",
    "    }\n",
    "\n",
    "evaluate_fertility = split_dataset[\"train\"].map(count_tokens, \n",
    "                                      batched=True,\n",
    "                                      remove_columns=[\"text\"], \n",
    "                                      num_proc=cpu_count()\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e42d466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3668402771 2388707891\n",
      "fertility: 1.5357268190143891\n"
     ]
    }
   ],
   "source": [
    "total_generated = sum(evaluate_fertility[\"generated\"])\n",
    "total_original = sum(evaluate_fertility[\"original\"])\n",
    "\n",
    "print(total_generated, total_original)\n",
    "\n",
    "print(\"fertility:\", total_generated/total_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2fb106",
   "metadata": {},
   "source": [
    "## Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77c5a027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=32768, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length = context_size\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8ef819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer will keep only: 512 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcebaf11a764f80938f426b16a7bed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/56726693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bff199f245f4e99990842e1564197fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/6302966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 56726693\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 6302966\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"The tokenizer will keep only: {context_size} tokens\" )\n",
    "\n",
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        max_length=context_size,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# preprocess dataset\n",
    "\n",
    "tokenized_datasets = split_dataset.map(group_texts, \n",
    "                                      batched=True,\n",
    "                                      remove_columns=[\"text\"], \n",
    "                                      num_proc=cpu_count()\n",
    "                                      )\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbdb2127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first quartile:\n",
      "[2, 5, 7, 8, 209, 9, 631, 281, 102, 5, 6, 325, 618, 108, 106, 9, 4870, 10, 335, 13172, 1532, 138, 5, 8, 10, 8553, 5, 6, 11693, 5, 11, 1436, 14, 4076, 10, 65, 7, 155, 10, 5]\n",
      "[CLS] ao final de maio deste ano , quase 100 pessoas foram detidas numa investida global contra os criadores , vendedores e usu√°rios do blackshades \n",
      "\n",
      "[2, 17, 7876, 127, 8681, 5, 6, 2385, 5, 20, 26, 29, 6289, 5, 11, 18, 912, 4149, 28, 7247, 3862, 9, 53, 1034, 19, 5, 7, 605, 9, 215, 5, 7, 10, 451, 5, 6, 2055, 5, 6, 6948]\n",
      "[CLS] para celebrar este acontecimento , realizou - se uma celebra√ß√£o eucar√≠stica na catedral metropolitana de pelotas com a presen√ßa de todas as crian√ßas , familiares , educadores\n",
      "\n",
      "[2, 5, 20, 5, 31, 4558, 5, 30, 5, 8, 991, 1344, 5, 12, 113, 5, 10, 7, 60, 145, 9332, 5, 150, 11, 34, 854, 1406, 5, 8, 1974, 15, 107, 14, 49, 13, 7261, 3627, 5, 8, 899]\n",
      "[CLS] - ( off ) o rei continua . assim sag√≠nero jeiper representa o come√ßo da vida do ser que queira conquistar o auto\n",
      "\n",
      "[2, 354, 9, 5, 7, 1712, 14, 6187, 49, 1224, 5, 6, 5, 7, 460, 2485, 7160, 71, 5, 7, 2061, 14, 500, 15, 8143, 12860, 24, 1929, 14, 6187, 5, 12, 3, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] apesar de a identidade do suspeito ser conhecida , a pol√≠cia pediu sigilo at√© a divulga√ß√£o do resultado da per√≠cia efetuada no computador do suspeito .[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "\n",
      "[2, 28, 863, 14, 83, 453, 9, 625, 9, 5, 5356, 5, 6, 5, 16139, 347, 5906, 43, 12288, 35, 8, 33, 21, 14147, 691, 5037, 14, 1265, 1285, 5, 46, 5, 22, 313, 5, 1564, 250, 34, 5, 34]\n",
      "[CLS] na manh√£ do dia 25 de setembro de 1968 , am√©rico boavida foi vitimado por um bombardeamento a√©reo do ex√©rcito portugu√™s √† \" base han√≥i i\n",
      "\n",
      "[2, 7382, 15529, 2050, 5, 7, 947, 5, 6, 133, 1643, 6518, 103, 2818, 17, 10058, 5, 7, 753, 9, 6086, 5, 6065, 5, 31, 10585, 5, 30, 5, 7, 8, 154, 14, 111, 5, 12, 3, 0, 0, 0]\n",
      "[CLS] m√°rcio jerry saiu a pedido , pois pretende dedicar tempo integral para coordenar a campanha de fl√°vio dino ( pcdob ) ao governo do estado .[SEP][PAD][PAD][PAD]\n",
      "\n",
      "[2, 5, 22, 938, 10521, 5, 8, 10, 11121, 10, 24, 2167, 5, 6, 47, 29, 353, 5, 25, 1263, 5, 50, 938, 4234, 91, 5, 8, 10, 3773, 10, 1465, 5, 6, 13, 1593, 10958, 73, 38, 11532, 10]\n",
      "[CLS] \" vamos aguardar os desdobramentos no jap√£o , mas uma coisa √© certa : vamos adotar todos os protocolos internacionais , que provavelmente ficar√£o muito mais rigorosos\n",
      "\n",
      "[2, 318, 21, 735, 5, 7, 8, 650, 15, 355, 5, 6, 16, 6865, 6236, 61, 5, 7, 421, 13, 26, 899, 5, 20, 7680, 5, 11, 5, 8, 5, 15372, 2216, 14, 7184, 185, 5, 50, 3, 0, 0]\n",
      "[CLS] diz um deles ao autor da pesquisa , em not√°vel contraste entre a cultura que se auto - atribui e o prec√°rio dom√≠nio do idioma nacional :[SEP][PAD][PAD]\n",
      "\n",
      "[2, 1479, 5, 8, 10, 2320, 1644, 5, 6, 5, 8, 5556, 5, 20, 3388, 2180, 9445, 11974, 5, 11, 5, 8, 6071, 3279, 9, 1975, 4948, 5, 14838, 5, 8, 508, 7177, 3281, 5, 11, 5400, 5, 12, 3]\n",
      "[CLS] dentre os oficiais participantes , o tenente - coronel andr√© mendon√ßa siqueira e o major rodrigo de lima gon√ßalves conclu√≠ram o curso avan√ßado operacional e estrat√©gico .[SEP]\n",
      "\n",
      "[2, 21, 1656, 15, 5, 8, 37, 18, 9, 1202, 2735, 9, 96, 15776, 13, 5, 7, 1174, 14, 148, 405, 5, 25, 2249, 17, 1697, 5, 7, 3649, 61, 5, 8, 10, 12125, 304, 9, 2593, 28, 1021, 1837]\n",
      "[CLS] um relat√≥rio da onu de 2005 explicou de forma convincente que a manuten√ß√£o do meio ambiente √© chave para reduzir a pobreza entre os 750 milh√µes de pobres na zona rural\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"first quartile:\")\n",
    "\n",
    "for elem in tokenized_datasets[\"train\"][:10]['input_ids']:\n",
    "    print(elem[:40])\n",
    "    print(tokenizer.decode(elem[:40]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1279d7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work1/lgarcia/renneruan/data/tokenized-for-training/custom/vocab_size:32_768/context_size:512'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets_name = os.path.join(DATA_FOLDER, f\"tokenized-for-training/custom/vocab_size:{vocabulary_size:_}/context_size:{context_size}\")\n",
    "\n",
    "tokenized_datasets_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd54878f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f36420e6b042fa92b88829c857d6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/409 shards):   0%|          | 0/56726693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a65530e3df429280dace55d065c58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/46 shards):   0%|          | 0/6302966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets.save_to_disk(tokenized_datasets_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c65fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c261f7397344919a70621f96accde7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=128):   0%|          | 0/56726693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bea71ec1b045e69581e90fca268fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=128):   0%|          | 0/6302966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 56686246\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 6298512\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def filtering(example):\n",
    "#     flags = []\n",
    "\n",
    "#     for id_list in example[\"input_ids\"]:\n",
    "\n",
    "#         if id_list[-1] != 0: # last token is not a padding (the doc was probably truncated)\n",
    "#             flags.append(False)\n",
    "\n",
    "#         elif id_list[10] == 0: # the token in the first 10 is a padding [PAD] (the doc is <= 10 tokens, including [sep])\n",
    "#             flags.append(False)\n",
    "\n",
    "#         else:\n",
    "#             flags.append(True)\n",
    "\n",
    "#     return flags\n",
    "\n",
    "\n",
    "# filtered_datasets = tokenized_datasets.filter(filtering,\n",
    "#                                          batched = True,\n",
    "#                                          num_proc = cpu_count(),\n",
    "#                                         )\n",
    "\n",
    "# filtered_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769cec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work1/lgarcia/renneruan/data/filtered/custom/vocab_size:32_768/context_size:512'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered_datasets_name = os.path.join(DATA_FOLDER, f\"filtered/custom/vocab_size:{vocabulary_size:_}/context_size:{context_size}\")\n",
    "# filtered_datasets_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cac6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save tokenized dataset locally:\n",
    "# filtered_datasets.save_to_disk(filtered_datasets_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-env",
   "language": "python",
   "name": "bert-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
