{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12468353",
   "metadata": {},
   "source": [
    "# Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c9390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k003-002.hpcfund\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b54e7979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work1/lgarcia/renneruan/cached_data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "WORK_DIR = os.getenv('WORK')\n",
    "\n",
    "DATA_FOLDER = os.path.join(WORK_DIR, \"data\")\n",
    "\n",
    "CACHED_DATA_FOLDER = os.path.join(WORK_DIR, \"cached_data\")\n",
    "\n",
    "# Salvamos o path do Cache par ao HuggingFace\n",
    "os.environ['HF_HOME'] = CACHED_DATA_FOLDER\n",
    "\n",
    "CACHED_DATA_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a01abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work1/lgarcia/renneruan\n"
     ]
    }
   ],
   "source": [
    "os.chdir(WORK_DIR)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13376096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers.normalizers import NFC, Lowercase, Replace\n",
    "from tokenizers import (\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    Tokenizer\n",
    ")\n",
    "\n",
    "from tokenizers.pre_tokenizers import Punctuation, Metaspace, WhitespaceSplit, Digits\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.decoders import Metaspace\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438b2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 32_768\n",
    "context_size = 512\n",
    "tokenizer_name = f\"tokenizers/custom/{vocabulary_size:_}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ccf8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_save_path = os.path.join(DATA_FOLDER, 'split_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54258c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = load_from_disk(split_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a25d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered by the index\n",
    "custom_special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3e69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc3\\x85' b'\\xe2\\x84\\xab'\n",
      "b'oi, \\xc3\\xa5 \\xc3\\xa5 tudo bem? o \"qu\\xc3\\xaa\" \"essa\" \"fun\\xc3\\xa7\\xc3\\xa3o\" faz?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'oi, å å tudo bem? o \"quê\" \"essa\" \"função\" faz?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace latex usage of \"aspas\"\n",
    "# NFC - Canonico (evita equivalencia) e composto, menos characteres\n",
    "# lowercase - unifica maiuscula e mi nusculo, que geralmente muda pouco (o positional embedding pode cuidar de comecar com maiuscula)\n",
    "\n",
    "# NFC, NFK\n",
    "# Ã A~\n",
    "\n",
    "custom_normalizer = normalizers.Sequence([\n",
    "    Replace(\"``\", '\"'), \n",
    "    Replace(\"''\", '\"'),\n",
    "    NFC(), \n",
    "    Lowercase(),\n",
    "])\n",
    "\n",
    "print(\"Å\".encode(\"utf-8\"), \"Å\".encode(\"utf-8\"))\n",
    "\n",
    "normalized_sample = custom_normalizer.normalize_str(\"Oi, Å Å tudo BEM? O ``quê'' \\\"essa\\\" ''função'' faz?\")\n",
    "\n",
    "print(normalized_sample.encode(\"utf-8\"))\n",
    "\n",
    "normalized_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9990d201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁Quer', (0, 4)),\n",
       " ('▁saber', (5, 10)),\n",
       " ('▁,', (10, 11)),\n",
       " ('▁vamos', (12, 17)),\n",
       " ('▁ver', (18, 21)),\n",
       " ('▁o', (22, 23)),\n",
       " ('▁que', (24, 27)),\n",
       " ('▁esse', (28, 32)),\n",
       " ('▁pre', (33, 36)),\n",
       " ('▁-', (36, 37)),\n",
       " ('▁tokenizer', (37, 46)),\n",
       " ('▁faz', (47, 50)),\n",
       " ('▁com', (51, 54)),\n",
       " ('▁numeros', (55, 62)),\n",
       " ('▁:', (62, 63)),\n",
       " ('▁12345', (64, 69)),\n",
       " ('▁123', (70, 73)),\n",
       " ('▁192', (74, 77)),\n",
       " ('▁193', (78, 81)),\n",
       " ('▁!', (81, 82)),\n",
       " ('▁?', (82, 83)),\n",
       " ('▁!', (83, 84))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# to keep the original words (separed by whitespaces), we use metaspaces, represented by:\n",
    "# “▁” U+2581 Lower One Eighth Block Unicode Character\n",
    "\n",
    "# Digits(individual_digits=True), Whitespace()\n",
    "\n",
    "custom_pre_tokenizer = pre_tokenizers.Sequence([WhitespaceSplit(), Punctuation(), Digits(individual_digits=False), Metaspace(replacement=\"▁\", prepend_scheme=\"always\")])\n",
    "\n",
    "custom_pre_tokenizer.pre_tokenize_str(\"Quer saber, vamos ver o que esse pre-tokenizer faz com numeros: 12345 123 192 193!?!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17c5cf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁o', (0, 1)),\n",
       " ('▁quê', (2, 5)),\n",
       " ('▁ele', (6, 9)),\n",
       " ('▁faz', (10, 13)),\n",
       " ('▁com', (16, 19)),\n",
       " ('▁espaços', (28, 35)),\n",
       " ('▁adicionais', (36, 46)),\n",
       " ('▁e', (50, 51)),\n",
       " ('▁quebras', (52, 59)),\n",
       " ('▁de', (60, 62)),\n",
       " ('▁linhas', (63, 69)),\n",
       " ('▁?', (69, 70))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_pre_tokenizer.pre_tokenize_str(\"o quê ele faz \\t com         espaços adicionais \\n\\n e quebras de linhas?   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1078d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_post_processor = TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", custom_special_tokens.index(\"[CLS]\")), (\"[SEP]\", custom_special_tokens.index(\"[SEP]\"))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de7cc10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_decoder = Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tokenizer = Tokenizer(Unigram())\n",
    "\n",
    "custom_tokenizer.normalizer = custom_normalizer\n",
    "custom_tokenizer.pre_tokenizer = custom_pre_tokenizer\n",
    "custom_tokenizer.post_processor = custom_post_processor\n",
    "custom_tokenizer.decoder = custom_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45309cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 443178/443178 [04:40<00:00, 1577.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_trainer = UnigramTrainer(\n",
    "        vocab_size=vocabulary_size,\n",
    "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "        special_tokens=custom_special_tokens,\n",
    "        unk_token=\"[UNK]\",\n",
    ")\n",
    "\n",
    "# create a python generator to dynamically load the data, one batch at a time\n",
    "def batch_iterator(batch_size=128): # 128 (cores)\n",
    "    for i in tqdm(range(0, len(split_dataset[\"train\"]), batch_size)):\n",
    "        yield split_dataset[\"train\"][i : i + batch_size][\"text\"]\n",
    "\n",
    "custom_tokenizer.train_from_iterator(iterator=batch_iterator(), trainer=custom_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dce87b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_custom_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=custom_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    bos_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    eos_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    padding_side=\"right\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af7673d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizers/custom/32_768/tokenizer_config.json',\n",
       " 'tokenizers/custom/32_768/special_tokens_map.json',\n",
       " 'tokenizers/custom/32_768/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save locally\n",
    "fast_custom_tokenizer.save_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9aeff8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = fast_custom_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1dbc70",
   "metadata": {},
   "source": [
    "## Teste do Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0189c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁olá', '▁pessoal', '▁', ',', '▁como', '▁vocês', '▁estão', '▁', '😁', '▁', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# print( tokenizer_albert.convert_ids_to_tokens( tokenizer_albert.encode(\"Olá pessoal, Como vocês estão 😁 ?\") ) )\n",
    "\n",
    "print( tokenizer.convert_ids_to_tokens( tokenizer.encode(\"Olá pessoal, Como vocês estão 😁 ?\") ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31514bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 7, 16208, 5, 8, 2535, 24, 5553, 5, 6, 47, 5, 8, 5553, 32, 2151, 3]\n",
      "['[CLS]', '▁', 'a', 'tiraram', '▁', 'o', '▁pau', '▁no', '▁gato', '▁', ',', '▁mas', '▁', 'o', '▁gato', '▁não', '▁morreu', '[SEP]']\n",
      "\n",
      "[2, 32, 774, 5, 6, 104, 774, 13, 43, 113, 5, 12, 5, 12, 5, 12, 3]\n",
      "['[CLS]', '▁não', '▁sei', '▁', ',', '▁só', '▁sei', '▁que', '▁foi', '▁assim', '▁', '.', '▁', '.', '▁', '.', '[SEP]']\n",
      "\n",
      "[2, 5, 16436, 5, 8, 427, 12153, 5, 6, 5, 11, 6102, 5, 8, 427, 587, 10929, 1272, 5, 31, 13588, 5, 30, 3]\n",
      "['[CLS]', '▁', 'testando', '▁', 'o', '▁modo', '▁continuo', '▁', ',', '▁', 'e', '▁tambem', '▁', 'o', '▁modo', '▁sub', 'jun', 'tivo', '▁', '(', '▁soubesse', '▁', ')', '[SEP]']\n",
      "\n",
      "[2, 3563, 5, 6, 4058, 5, 6, 378, 5, 6, 13011, 5, 6, 5, 19156, 5, 6, 2229, 5, 6, 480, 3]\n",
      "['[CLS]', '▁justo', '▁', ',', '▁justa', '▁', ',', '▁justiça', '▁', ',', '▁injusto', '▁', ',', '▁', 'injustamente', '▁', ',', '▁justamente', '▁', ',', '▁junto', '[SEP]']\n",
      "\n",
      "[2, 5, 16436, 5, 7, 2211, 8, 10, 5, 6, 178, 13, 644, 5, 8, 10, 5, 7, 2211, 8, 10, 161, 7, 5, 7, 5, 7, 2274, 9503, 14, 189, 535, 5, 101, 3]\n",
      "['[CLS]', '▁', 'testando', '▁', 'a', 'cent', 'o', 's', '▁', ',', '▁será', '▁que', '▁manter', '▁', 'o', 's', '▁', 'a', 'cent', 'o', 's', '▁melhor', 'a', '▁', 'a', '▁', 'a', 'cur', 'ácia', '▁do', '▁meu', '▁modelo', '▁', '?', '[SEP]']\n",
      "\n",
      "[2, 1008, 5, 50, 5, 7, 23, 34, 9678, 5, 6, 2915, 5, 6, 5, 7, 23, 34, 13257, 5, 6, 5, 7, 5185, 158, 5, 6, 2915, 780, 8, 5, 6, 2915, 2326, 780, 8, 3]\n",
      "['[CLS]', '▁amigo', '▁', ':', '▁', 'a', 'm', 'i', 'guinho', '▁', ',', '▁amiga', '▁', ',', '▁', 'a', 'm', 'i', 'guinha', '▁', ',', '▁', 'a', 'mig', 'ão', '▁', ',', '▁amiga', 'ç', 'o', '▁', ',', '▁amiga', 'lha', 'ç', 'o', '[SEP]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_tokenizer(sample):\n",
    "    encoding = tokenizer.encode(sample)\n",
    "    print(encoding)\n",
    "    print(tokenizer.convert_ids_to_tokens(encoding))\n",
    "    print()\n",
    "\n",
    "test_tokenizer('''\n",
    "Atiraram o pau no gato, mas o gato não morreu\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "Não sei, só sei que foi assim...\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "Testando o modo continuo, e tambem o modo subjuntivo ( soubesse )\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "justo, justa, justiça, injusto, injustamente, justamente, junto\n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "    testando acentos, será que manter os acentos melhora a acurácia do meu modelo? \n",
    "''')\n",
    "\n",
    "test_tokenizer('''\n",
    "    amigo: amiguinho, amiga, amiguinha, amigão, amigaço, amigalhaço\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d7db67",
   "metadata": {},
   "source": [
    "#### Objetively Evaluate Tokenizer on Compression Rate (1/Fertility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "478ee47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁ao', (0, 2)),\n",
       " ('▁final', (3, 8)),\n",
       " ('▁de', (9, 11)),\n",
       " ('▁maio', (12, 16)),\n",
       " ('▁deste', (17, 22)),\n",
       " ('▁ano', (23, 26)),\n",
       " ('▁,', (26, 27)),\n",
       " ('▁quase', (28, 33)),\n",
       " ('▁100', (34, 37)),\n",
       " ('▁pessoas', (38, 45)),\n",
       " ('▁foram', (46, 51)),\n",
       " ('▁detidas', (52, 59)),\n",
       " ('▁numa', (60, 64)),\n",
       " ('▁investida', (65, 74)),\n",
       " ('▁global', (75, 81)),\n",
       " ('▁contra', (82, 88)),\n",
       " ('▁os', (89, 91)),\n",
       " ('▁criadores', (92, 101)),\n",
       " ('▁,', (101, 102)),\n",
       " ('▁vendedores', (103, 113)),\n",
       " ('▁e', (114, 115)),\n",
       " ('▁usuários', (116, 124)),\n",
       " ('▁do', (125, 127)),\n",
       " ('▁blackshades', (128, 139)),\n",
       " ('▁rat', (140, 143)),\n",
       " ('▁.', (143, 144))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_and_pre_tokenize(text):\n",
    "    normalized = tokenizer.backend_tokenizer.normalizer.normalize_str(text)\n",
    "    processed = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(normalized)\n",
    "    return processed\n",
    "\n",
    "normalize_and_pre_tokenize( split_dataset[\"train\"][0][\"text\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dabcc092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b534ced9464125a57137d151d0b4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/56726693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def count_tokens(batch):\n",
    "    \n",
    "    original_tokens = 0\n",
    "    generated_tokens = 0\n",
    "\n",
    "    for doc in batch[\"text\"]:\n",
    "\n",
    "        original_tokens += len( normalize_and_pre_tokenize(doc) )\n",
    "        generated_tokens += len( tokenizer.encode(doc) )\n",
    "        \n",
    "    # Add the token counts as a new column to the batch\n",
    "    return {\n",
    "        \"generated\": [generated_tokens],\n",
    "        \"original\": [original_tokens]\n",
    "    }\n",
    "\n",
    "evaluate_fertility = split_dataset[\"train\"].map(count_tokens, \n",
    "                                      batched=True,\n",
    "                                      remove_columns=[\"text\"], \n",
    "                                      num_proc=cpu_count()\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e42d466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3668402771 2388707891\n",
      "fertility: 1.5357268190143891\n"
     ]
    }
   ],
   "source": [
    "total_generated = sum(evaluate_fertility[\"generated\"])\n",
    "total_original = sum(evaluate_fertility[\"original\"])\n",
    "\n",
    "print(total_generated, total_original)\n",
    "\n",
    "print(\"fertility:\", total_generated/total_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2fb106",
   "metadata": {},
   "source": [
    "## Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77c5a027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=32768, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length = context_size\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8ef819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer will keep only: 512 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcebaf11a764f80938f426b16a7bed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/56726693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bff199f245f4e99990842e1564197fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/6302966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 56726693\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 6302966\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"The tokenizer will keep only: {context_size} tokens\" )\n",
    "\n",
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        max_length=context_size,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# preprocess dataset\n",
    "\n",
    "tokenized_datasets = split_dataset.map(group_texts, \n",
    "                                      batched=True,\n",
    "                                      remove_columns=[\"text\"], \n",
    "                                      num_proc=cpu_count()\n",
    "                                      )\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbdb2127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first quartile:\n",
      "[2, 5, 7, 8, 209, 9, 631, 281, 102, 5, 6, 325, 618, 108, 106, 9, 4870, 10, 335, 13172, 1532, 138, 5, 8, 10, 8553, 5, 6, 11693, 5, 11, 1436, 14, 4076, 10, 65, 7, 155, 10, 5]\n",
      "[CLS] ao final de maio deste ano , quase 100 pessoas foram detidas numa investida global contra os criadores , vendedores e usuários do blackshades \n",
      "\n",
      "[2, 17, 7876, 127, 8681, 5, 6, 2385, 5, 20, 26, 29, 6289, 5, 11, 18, 912, 4149, 28, 7247, 3862, 9, 53, 1034, 19, 5, 7, 605, 9, 215, 5, 7, 10, 451, 5, 6, 2055, 5, 6, 6948]\n",
      "[CLS] para celebrar este acontecimento , realizou - se uma celebração eucarística na catedral metropolitana de pelotas com a presença de todas as crianças , familiares , educadores\n",
      "\n",
      "[2, 5, 20, 5, 31, 4558, 5, 30, 5, 8, 991, 1344, 5, 12, 113, 5, 10, 7, 60, 145, 9332, 5, 150, 11, 34, 854, 1406, 5, 8, 1974, 15, 107, 14, 49, 13, 7261, 3627, 5, 8, 899]\n",
      "[CLS] - ( off ) o rei continua . assim sagínero jeiper representa o começo da vida do ser que queira conquistar o auto\n",
      "\n",
      "[2, 354, 9, 5, 7, 1712, 14, 6187, 49, 1224, 5, 6, 5, 7, 460, 2485, 7160, 71, 5, 7, 2061, 14, 500, 15, 8143, 12860, 24, 1929, 14, 6187, 5, 12, 3, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] apesar de a identidade do suspeito ser conhecida , a polícia pediu sigilo até a divulgação do resultado da perícia efetuada no computador do suspeito .[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "\n",
      "[2, 28, 863, 14, 83, 453, 9, 625, 9, 5, 5356, 5, 6, 5, 16139, 347, 5906, 43, 12288, 35, 8, 33, 21, 14147, 691, 5037, 14, 1265, 1285, 5, 46, 5, 22, 313, 5, 1564, 250, 34, 5, 34]\n",
      "[CLS] na manhã do dia 25 de setembro de 1968 , américo boavida foi vitimado por um bombardeamento aéreo do exército português à \" base hanói i\n",
      "\n",
      "[2, 7382, 15529, 2050, 5, 7, 947, 5, 6, 133, 1643, 6518, 103, 2818, 17, 10058, 5, 7, 753, 9, 6086, 5, 6065, 5, 31, 10585, 5, 30, 5, 7, 8, 154, 14, 111, 5, 12, 3, 0, 0, 0]\n",
      "[CLS] márcio jerry saiu a pedido , pois pretende dedicar tempo integral para coordenar a campanha de flávio dino ( pcdob ) ao governo do estado .[SEP][PAD][PAD][PAD]\n",
      "\n",
      "[2, 5, 22, 938, 10521, 5, 8, 10, 11121, 10, 24, 2167, 5, 6, 47, 29, 353, 5, 25, 1263, 5, 50, 938, 4234, 91, 5, 8, 10, 3773, 10, 1465, 5, 6, 13, 1593, 10958, 73, 38, 11532, 10]\n",
      "[CLS] \" vamos aguardar os desdobramentos no japão , mas uma coisa é certa : vamos adotar todos os protocolos internacionais , que provavelmente ficarão muito mais rigorosos\n",
      "\n",
      "[2, 318, 21, 735, 5, 7, 8, 650, 15, 355, 5, 6, 16, 6865, 6236, 61, 5, 7, 421, 13, 26, 899, 5, 20, 7680, 5, 11, 5, 8, 5, 15372, 2216, 14, 7184, 185, 5, 50, 3, 0, 0]\n",
      "[CLS] diz um deles ao autor da pesquisa , em notável contraste entre a cultura que se auto - atribui e o precário domínio do idioma nacional :[SEP][PAD][PAD]\n",
      "\n",
      "[2, 1479, 5, 8, 10, 2320, 1644, 5, 6, 5, 8, 5556, 5, 20, 3388, 2180, 9445, 11974, 5, 11, 5, 8, 6071, 3279, 9, 1975, 4948, 5, 14838, 5, 8, 508, 7177, 3281, 5, 11, 5400, 5, 12, 3]\n",
      "[CLS] dentre os oficiais participantes , o tenente - coronel andré mendonça siqueira e o major rodrigo de lima gonçalves concluíram o curso avançado operacional e estratégico .[SEP]\n",
      "\n",
      "[2, 21, 1656, 15, 5, 8, 37, 18, 9, 1202, 2735, 9, 96, 15776, 13, 5, 7, 1174, 14, 148, 405, 5, 25, 2249, 17, 1697, 5, 7, 3649, 61, 5, 8, 10, 12125, 304, 9, 2593, 28, 1021, 1837]\n",
      "[CLS] um relatório da onu de 2005 explicou de forma convincente que a manutenção do meio ambiente é chave para reduzir a pobreza entre os 750 milhões de pobres na zona rural\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"first quartile:\")\n",
    "\n",
    "for elem in tokenized_datasets[\"train\"][:10]['input_ids']:\n",
    "    print(elem[:40])\n",
    "    print(tokenizer.decode(elem[:40]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1279d7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work1/lgarcia/renneruan/data/tokenized-for-training/custom/vocab_size:32_768/context_size:512'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets_name = os.path.join(DATA_FOLDER, f\"tokenized-for-training/custom/vocab_size:{vocabulary_size:_}/context_size:{context_size}\")\n",
    "\n",
    "tokenized_datasets_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd54878f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f36420e6b042fa92b88829c857d6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/409 shards):   0%|          | 0/56726693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a65530e3df429280dace55d065c58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/46 shards):   0%|          | 0/6302966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets.save_to_disk(tokenized_datasets_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c65fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c261f7397344919a70621f96accde7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=128):   0%|          | 0/56726693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bea71ec1b045e69581e90fca268fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=128):   0%|          | 0/6302966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 56686246\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 6298512\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def filtering(example):\n",
    "#     flags = []\n",
    "\n",
    "#     for id_list in example[\"input_ids\"]:\n",
    "\n",
    "#         if id_list[-1] != 0: # last token is not a padding (the doc was probably truncated)\n",
    "#             flags.append(False)\n",
    "\n",
    "#         elif id_list[10] == 0: # the token in the first 10 is a padding [PAD] (the doc is <= 10 tokens, including [sep])\n",
    "#             flags.append(False)\n",
    "\n",
    "#         else:\n",
    "#             flags.append(True)\n",
    "\n",
    "#     return flags\n",
    "\n",
    "\n",
    "# filtered_datasets = tokenized_datasets.filter(filtering,\n",
    "#                                          batched = True,\n",
    "#                                          num_proc = cpu_count(),\n",
    "#                                         )\n",
    "\n",
    "# filtered_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769cec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work1/lgarcia/renneruan/data/filtered/custom/vocab_size:32_768/context_size:512'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered_datasets_name = os.path.join(DATA_FOLDER, f\"filtered/custom/vocab_size:{vocabulary_size:_}/context_size:{context_size}\")\n",
    "# filtered_datasets_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cac6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save tokenized dataset locally:\n",
    "# filtered_datasets.save_to_disk(filtered_datasets_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-env",
   "language": "python",
   "name": "bert-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
